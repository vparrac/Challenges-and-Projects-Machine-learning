{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1\n",
    "## Dependencies\n",
    "To run this code is necessary numpy and sklearn libraries.\n",
    "## Description\n",
    "The objective of this challenge is classify data in two classes: Jazz & Blues and Soul and Reggae using a logistic regresión.\n",
    "## Data cleansing\n",
    "Before loading the data into Python, it was necessary to clean the data set. For this, the set of songs belonging to different musical genres to _Jazz & Blues_ or _Soul and Reaggae_ was eliminated. String attributes were also deleted. Then, the representation of the classes was changed, \"0\" was selected to represent the _Jazz & Blues_ class and \"1\" to represent the Alma and Reagge class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import nunpy to read data\n",
    "import numpy as np\n",
    "# SKLearn is used to make the logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# In order to split randomply train_test_split method (of SKLearn) will be user\n",
    "from sklearn.model_selection import train_test_split\n",
    "# To evalue the results a confusión matrix and ACA were used\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score \n",
    "#Import data\n",
    "data = np.genfromtxt('clean_data.csv', delimiter=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After, it is separate the class of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels= data[:,0]\n",
    "features = data[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part one: Train the algorithm using all data\n",
    "This challenge is divided in four parts. In the first part all the data will be used. To separte the training data of test data the train_test_split function of SKLear was used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the training set, we can create the logistic regression. The solver parameter was fixed in sag in order to facilitate the convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver = \"sag\", max_iter=100000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model created we can predict the new classes with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the results we will use the confusion matrix (CM) and the accuracy score (ACA). \n",
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"2\" rowspan=\"2\"></th>\n",
    "    <th colspan=\"2\">Actual class<br></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"2\"><br>Predicted</td>\n",
    "    <td>0<br></td>\n",
    "    <td><span style=\"color:rgb(51, 51, 51)\">TP</span></td>\n",
    "    <td>FP<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>FN</td>\n",
    "    <td>TN</td>\n",
    "  </tr>\n",
    "</table>\n",
    "The ACA number is defined as following equation and its representation the average of data that was predicted in the correct class divided in  the number of records predicted for this class.\n",
    "\n",
    "$ACA= \\frac{1}{2}\\frac{TP}{TP+FP}+\\frac{1}{2}\\frac{TN}{FN+TN}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[733 150]\n",
      " [132 655]]\n",
      "0.8311377245508982\n"
     ]
    }
   ],
   "source": [
    "matrix = confusion_matrix(y_test, predictions) \n",
    "ACA = accuracy_score(y_test, predictions) \n",
    "print(matrix)\n",
    "print(ACA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part two: Train the algorithm using 50% of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the half-data we separate the set again into training and test sets.\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X_train, y_train, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = LogisticRegression(solver = \"sag\", max_iter=100000).fit(X2_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = clf2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix 50% of data\n",
      "[[731 152]\n",
      " [132 655]]\n",
      "ACA 50% of data\n",
      "0.829940119760479\n"
     ]
    }
   ],
   "source": [
    "matrix2 = confusion_matrix(y_test, predictions2) \n",
    "ACA2 = accuracy_score(y_test, predictions2) \n",
    "print('Confusion matrix 50% of data')\n",
    "print(matrix2)\n",
    "print('ACA 50% of data')\n",
    "print(ACA2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part three: Train the algorithm using 20% of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the half-data we separate the set again into training and test sets.\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X_train, y_train, test_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = LogisticRegression(solver = \"sag\", max_iter=100000).fit(X3_train, y3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions3 = clf3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix 20% of data\n",
      "[[735 148]\n",
      " [142 645]]\n",
      "ACA 20% of data\n",
      "0.8263473053892215\n"
     ]
    }
   ],
   "source": [
    "matrix3 = confusion_matrix(y_test, predictions3) \n",
    "ACA3 = accuracy_score(y_test, predictions3) \n",
    "print('Confusion matrix 20% of data')\n",
    "print(matrix3)\n",
    "print('ACA 20% of data')\n",
    "print(ACA3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part four: Train the algorithm using 10% of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X_train, y_train, test_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4 = LogisticRegression(solver = \"sag\", max_iter=100000).fit(X4_train, y4_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions4 = clf4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix 10% of data\n",
      "[[706 177]\n",
      " [121 666]]\n",
      "ACA 10% of data\n",
      "0.8215568862275449\n"
     ]
    }
   ],
   "source": [
    "matrix4 = confusion_matrix(y_test, predictions4) \n",
    "ACA4 = accuracy_score(y_test, predictions4) \n",
    "print('Confusion matrix 10% of data')\n",
    "print(matrix4)\n",
    "print('ACA 10% of data')\n",
    "print(ACA4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACA numbers for each model\n",
      "0.8311377245508982 0.829940119760479 0.8263473053892215 0.8215568862275449\n"
     ]
    }
   ],
   "source": [
    "print('ACA numbers for each model')\n",
    "print(ACA,ACA2,ACA3,ACA4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ACA number almost no has variation between models. This facts sugest that or our data is very linear or every class is very grouped, for this reason the accuracy of model is not improved significaly with the among of data, a small set is significant enough. In general terms the models have a good accuracy and we can say that the objective of the challenge was accomplished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valerie Parra Cortés\n",
    "201619703"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
